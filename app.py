# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j-pCUb9Y2fJdVnvdNe9GzRKmZfjr5kw6
"""

# ✅ 1. Install required libraries
!pip install transformers datasets scikit-learn --quiet

# ✅ 2. Import libraries
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer
from transformers import DataCollatorWithPadding
from sklearn.metrics import accuracy_score, precision_recall_fscore_support



# Replace with your actual file name after upload
df = pd.read_csv("Expressive_Balanced_Dataset.csv")

# ✅ 4. Train/Validation Split
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["text"].tolist(), df["label"].tolist(), test_size=0.2, random_state=42
)

# ✅ 5. Tokenization
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

# ✅ 6. Prepare Torch Datasets
class DepressionDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __len__(self):
        return len(self.labels)
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_dataset = DepressionDataset(train_encodings, train_labels)
val_dataset = DepressionDataset(val_encodings, val_labels)

# ✅ 7. Load BERT for binary classification
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# ✅ 8. Define evaluation metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=1)
    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# ✅ 9. Training setup
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

# ✅ 10. Start training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
)

trainer.train()

from google.colab import drive
drive.mount('/content/drive')

model_path = "/content/drive/MyDrive/empaAI_bert_model"

model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

from transformers import BertForSequenceClassification, BertTokenizer

model = BertForSequenceClassification.from_pretrained(model_path)
tokenizer = BertTokenizer.from_pretrained(model_path)

def classify_text(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    label = torch.argmax(probs, dim=1).item()
    confidence = probs[0][label].item()
    return "Depressed" if label == 1 else "Happy", confidence

# Example
text = "Great day, soo happy."
result, score = classify_text(text)
print(f"Prediction: {result}, Confidence: {score:.2f}")

"""**`GPT2 CODE`**"""

!pip install transformers sentencepiece --quiet

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

chat_tokenizer = AutoTokenizer.from_pretrained("microsoft/GODEL-v1_1-base-seq2seq")
chat_model = AutoModelForSeq2SeqLM.from_pretrained("microsoft/GODEL-v1_1-base-seq2seq")

!pip install transformers sentencepiece streamlit-chat --quiet

from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import streamlit as st
import torch

# Load BERT depression classifier (already loaded earlier in your app)
bert_model_path = "/content/drive/MyDrive/empaAI_bert_model"
bert_model = BertForSequenceClassification.from_pretrained(bert_model_path)
bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)

# ✅ Load GODEL chatbot model
chat_tokenizer = AutoTokenizer.from_pretrained("microsoft/GODEL-v1_1-base-seq2seq")
chat_model = AutoModelForSeq2SeqLM.from_pretrained("microsoft/GODEL-v1_1-base-seq2seq")

# Session state for chat
if "past" not in st.session_state:
    st.session_state.past = []
if "generated" not in st.session_state:
    st.session_state.generated = []

# Depression classifier
def detect_depression(text):
    inputs = bert_tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    outputs = bert_model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    label = torch.argmax(probs, dim=1).item()
    return label, probs[0][label].item()

def chatbot_reply(user_input, label):
    context = user_input.strip()
    if not context:
        return "I'm here whenever you want to talk. 😊"

    if label == 1:
        prompt = (
            "Instruction: You are a compassionate and supportive chatbot trained to help users who feel low or emotionally distressed.\n"
            f"Context: {context}\n"
            "Response:"
        )
    else:
        prompt = (
            "Instruction: You are a cheerful and friendly chatbot. Respond with positivity and encouragement.\n"
            f"Context: {context}\n"
            "Response:"
        )

    input_ids = chat_tokenizer.encode(prompt, return_tensors="pt", truncation=True)
    output_ids = chat_model.generate(
        input_ids,
        max_length=100,
        do_sample=True,
        top_p=0.9,
        temperature=0.6,
        num_return_sequences=1,
        repetition_penalty=1.2  # ✅ Prevent looping responses
    )

    response = chat_tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()

    # Remove echoed prompt if present
    if "Response:" in response:
        response = response.split("Response:")[-1].strip()

    if not response:
        response = "Thanks for opening up. I'm here for you." if label == 1 else "Sounds great! 😊"

    response += " 💙" if label == 1 else " 😊"
    return response






# Streamlit UI
st.title("💬 EmpaAI - Depression-Aware Chatbot")
st.markdown("This chatbot detects emotional tone and responds empathetically. It does **not** provide medical advice.")

user_input = st.text_input("You:", key="input")

import os

if user_input:
    label, confidence = detect_depression(user_input)
    response = chatbot_reply(user_input, label)

    if label == 1 and confidence > 0.85:
        response += "\n\n⚠️ Please consider talking to someone you trust or reaching out to NHS or Mind UK."

    st.session_state.past.append(user_input)
    st.session_state.generated.append(response)

    # ✅ Full logging to Drive from inside Streamlit app
    log_dir = "/content/drive/MyDrive/chat_logs"
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, "chat_log.csv")

    with open(log_path, "a", encoding="utf-8") as f:
        f.write(f'"{user_input}","{response}",{label},{confidence:.2f}\n')

    # Optional UI confirmation
    st.caption("💾 Logged to: " + log_path)




# Chat display
from streamlit_chat import message
if st.session_state.generated:
    for i in range(len(st.session_state.generated)):
        message(st.session_state.past[i], is_user=True, key=f"user_{i}")
        message(st.session_state.generated[i], key=f"bot_{i}")

!zip -r /content/drive/MyDrive/empaAI_backup.zip app.py chat_log.csv /content/drive/MyDrive/empaAI_bert_model

!pkill streamlit
!pkill ngrok

from pyngrok import ngrok
import threading
import time

def run_app():
    !streamlit run app.py --server.headless true

threading.Thread(target=run_app).start()
time.sleep(5)
print("🔗", ngrok.connect("http://localhost:8501"))